---
title: "Investigating Audio Features of Electronic Music Sub Genres with Advanced Statistical Methods and Modeling:"
output: html_document
author: Dylan Watson
date: "2024-05-06"
---

```{r setup, include=FALSE}
# Setup global rmd params
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Abstract:
---------
The goal of this study is to determine if track audio features from the Spotify API are capable of predicting the genre of music a track belongs to. In particular I will be investigating 7 unique sub genres of electronic music. To obtain my data set I will engineer the data myself utilizing the spotifyr wrapper class which calls the Spotify API. I'll produce genre specific inferences by creating boxplots and joyplots (stacked kernel density estimates (KDE)) of individual audio features by genre, use principal component analysis (PCA) and the rgl library to create interactive 3D visuals with concentration ellipsoids that cluster tracks by genre, demonstrate kmeans clustering, and create and evaluate a classification model for genre.

Import Libraries:
-----------------
- 3D plotting library options include plotly, plot3D, scatterplot3D, or rgl.
- rgl appears to be the most powerful interactive 3d ploting library.
- joyplot package options include ggridges or GGjoy.
- choosing GGjoy because it functions as ggplot() layer.
```{r, Install Packages, echo = FALSE}
#install.packages("spotifyr")
#install.packages("ggjoy")
#install.packages("factoextra")
#install.packages("rgl")
#install.packages("webshot2")
#install.packages("gridExtra")
#install.packages("cowplot")
#install.packages("xgboost")
#install.packages("tinytex")
```

```{r Import Packages}
library(spotifyr) # For spotify api wrapper functions in R
library(dplyr) # For data manipulation
library(ggplot2) # For plots
library(ggjoy) # For joyplots
library(rgl) # For Interactive 3D plotting w/ OpenGL
library(webshot2) # For saving rotating plot animation to gif
library(gridExtra) # For plotting to grid
library(cowplot) # For plotting to grid
library(RColorBrewer) # For color palette
library(xgboost) # For random forest multiple classification model
library(caret) # For test train split
```

Spotify API & Data Set Engineering:
-----------------------------------
Spotify API Sources: <br>
- https://developer.spotify.com/documentation/web-api <br>
Spotifyr Sources: <br>
- https://cran.r-project.org/web/packages/spotifyr/readme/README.html <br>
- https://cran.r-project.org/web/packages/spotifyr/spotifyr.pdf <br>
- https://github.com/charlie86/spotifyr#readme

API Access Token:
-----------------
Set up access to Spotify API using id and secret keys with get_spotify_access_token().
```{r Access Token}
# Don't use my keys <3
Sys.setenv(SPOTIFY_CLIENT_ID = 'bc3e8be6a8fb47838fcc7ad4c250fe3e')
Sys.setenv(SPOTIFY_CLIENT_SECRET = '5c06d84b9cae4143935ee57aae49e28f')
access_token = get_spotify_access_token()
```

Explore spotifyr Functions of Interest:
---------------------------------------
Looking through the documentation for the spotifyr library I selected possible functions of interest I could use to obtain the feature level track data I desire for the analysis. I ended up selecting get_playlist() to obtain metadata on the playlists that I would need for my second function call I selected which is get_playlist_audio_features(). In other explorations of the Spotify API such as mass data scraping there are many other functions here I have used. <br>

Authorization Functions:
- get_spotify_access_token()

Album Functions: <br>
- get_album()
- get_albums()

Artist Functions: <br>
- get_artist()
- get_artists()
- get_artist_albums()
- get_artist_top_tracks()
- get_related_artists()

Label Functions: <br>
- get_label_artists()

Musicology Functions: <br>
- get_artist_audio_features()
- get_genre_artists()
- get_playlist_audio_features()
- get_track_audio_analysis()
- get_track_audio_features()

Category Functions: <br>
- get_categories()
- get_category()
- get_category_playlists()

Search Functions: <br>
- search_spotify()

Track Functions: <br>
- get_album_tracks()
- get_playlist_tracks()
- get_track()
- get_tracks()

Playlist Functions: <br>
- get_featured_playlists()
- get_playlist()

Recommendations Functions: <br>
- get_recommendations()
- get_recommendations_all()

Exploring Color Palettes:
-------------------------
In the following cell I used a library called RColorBrewer to import pre-selected lists or color palettes to use for data visualization. RColorBrewer contains three types of palettes; quantitative, diverging, and qualitative. I chose to work with the Dark2 qualitative palette for my classification data. <br>

Source: <br>
- https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/ <br>
- https://colorbrewer2.org/#type=qualitative&scheme=Set2&n=7
```{r Explore RColorBrewer, echo = FALSE, eval = FALSE, fig.width = 8, fig.height = 6}
# Display all palettes
#display.brewer.all()

# Display qualitative palettes
#display.brewer.all(n = 7, type = "qual")
```

```{r Dark2 Palette, fig.width = 4, fig.height = 2}
# Display Dark2
display.brewer.pal(n = 7, name = "Dark2")
```

Input Playlists & Genres:
-------------------------
Here I hand selected 7 different playlist id's from Spotify which accurately contain and classify tracks for which we will use the playlist sub genre as the true response for the tracks it contains.
```{r Seed Data}
# Initialize playlist ids, genres, and plotting colors manually from Spotify app
# Add genres and playlists in alphabetical order because otherwise plots will reorder factors and lose color consistency across rmd
genre = c("DnB", "Dubstep", "Hardstyle", "House", "Riddim", "Techno", "Trance")
id = c('3gqEaRQUN0xYi9kHexWQpY', '37i9dQZF1DX5Q27plkaOQ3', '37i9dQZF1DX0pH2SQMRXnC', '6vDGVr652ztNWKZuHvsFvx', '5sXJsa3I49Tgs4WO1JRmbm', '18vUeZ9BdtMRNV6gI8RnR6', '37i9dQZF1DX91oIci4su1D')
color = brewer.pal(n = 7, name = "Dark2")
playlist_df = as.data.frame(cbind(genre, id, color))

# Check playlist_df
#View(playlist_df)
print(playlist_df)
```
```{r Import Data, echo = FALSE}
# Import data from previously written csv to save knit time.
playlists_info_df_clean = read.csv('playlists_metadata.csv')
playlists_tracks_features_df_clean = read.csv('track_features.csv')
```

Get Playlist Metadata:
----------------------
Once the desired playlists have been seeded into my starting dataframe I will use the get_playlist() function from spotifyr to get other important metadata and store in a new frame.
```{r Get Playlist Metadata, eval = FALSE}
# Initialize Empty Frame
playlists_info_df_clean = data.frame()

# Get more playlists info
for (row in 1:nrow(playlist_df)) {
  # Console Output
  #print(paste("Getting Playlist", row, "Info"))
  # API Call
  playlists_info_raw = get_playlist(playlist_df[row, "id"])
  # Data Munging
  playlists_info_df_clean = rbind(playlists_info_df_clean, data.frame("name" = playlists_info_raw$name, "owner" = playlists_info_raw$owner$display_name, "id" = playlists_info_raw$id, 
                                                                      "genre" = playlist_df[row, "genre"], "followers" = playlists_info_raw$followers[[2]], "total.tracks" = playlists_info_raw$tracks$total, "color" = playlist_df[row, "color"]
                                                                      )
                                  )
  # Sleep to avoid calling rate limits
  Sys.sleep(5)
}

# Write to csv if Spotify API stops working.
#write.csv(playlists_info_df_clean, "playlists_metadata.csv", row.names = FALSE)
```

```{r Print Playlists Metadata, echo = FALSE}
# Check playlists_info_df_clean
#View(playlists_info_df_clean)
print(playlists_info_df_clean)
```

Get Tracks from Playlists:
--------------------------
Using the playlist metadata dataframe, I use the spotifyr function get_playlist_tracks() to retrieve the songs for each playlist, clean it, and bind it to a new frame for tracks. *No longer needed, better function found in cell below.
```{r Get Tracks, echo = FALSE, eval = FALSE}
# Initialize empty frame for playlists tracks data
playlists_tracks_clean = data.frame()
for (row in 1:nrow(playlists_info_df_clean)) {
  # Console Output
  #print(paste("Getting Tracks for Playlist ", row))
  # Declare counter vars
  max_limit = 100
  offset = 0
  tracks_remaining = playlists_info_df_clean[row, "total.tracks"]

  # Loop till retrieved all songs from playlist
  while (tracks_remaining > 0) {
    # Console Output
    print(paste("Getting Tracks Call"))
    # Calc limit for API call
    if(tracks_remaining > 100) {
      limit = 100
    } else {
      limit = tracks_remaining
    }
    # Call API
    playlist_tracks_raw = get_playlist_tracks(playlists_info_df_clean[row, "id"], limit = limit, offset = offset)
    # Data Munging
    playlist_tracks_temp = as.data.frame(playlist_tracks_raw) |>
      select(track.artists, track.duration_ms, track.id, track.name, track.popularity) |>
      mutate(genre = playlists_info_df_clean[row, "genre"])
    # Bind to clean df
    playlists_tracks_clean = rbind(playlists_tracks_clean, playlist_tracks_temp)
    # Update counters
    offset = offset + limit
    tracks_remaining = tracks_remaining - limit
    # Wait to prevent api calling rate limits
    Sys.sleep(5)
  } # End while

} # End for 

# View Playlists Tracks Df
print(head(playlists_tracks_clean))
```

Get Track Audio Features:
-------------------------
Use playlist metadata dataframe with the spotifyr function get_playlist_audio_features() to get audio features of all tracks, clean, and bind to output to my final dataframe containing all the tracks and audio features by genre. In total, there are 1115 songs with 23 rows of descriptive features for each song.
```{r, Get Audio Features, eval = FALSE}
# Initialize empty df for track features
playlists_tracks_features_df_clean = data.frame()
for (row in 1:nrow(playlists_info_df_clean)) {
  # Console Output
  #print(paste("Getting Track Features for Playlist ", row))
  # Call API
  playlists_tracks_features_df_raw = get_playlist_audio_features(playlists_info_df_clean[row, "owner"], playlists_info_df_clean[row, "id"])
  # Data Munging
  playlists_tracks_features_df_temp = playlists_tracks_features_df_raw |>
  select(track.id, track.name, track.popularity, track.album.release_date, track.duration_ms, track.explicit, danceability, energy, key, 
         loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, key_name, mode_name, key_mode, time_signature
         ) |>
  mutate(genre = playlists_info_df_clean[row, "genre"], color = playlists_info_df_clean[row, "color"])
  # TODO: manipulate track.artists col to be comma separated artist names instead of list within col. Causes print and write issues. Not necessary for project.
  # Bind to output df
  playlists_tracks_features_df_clean = rbind(playlists_tracks_features_df_clean, playlists_tracks_features_df_temp)
  # Wait to prevent api calling rate limits
  Sys.sleep(5)
}

# Write to csv if Spotify API stops working.
#write.csv(playlists_tracks_features_df_clean, "track_features.csv", row.names = FALSE)
```

```{r Print Track Audio Features, echo = FALSE}
# Check playlists_tracks_features_df_clean 
#View(playlists_tracks_features_df_clean)
print(head(playlists_tracks_features_df_clean))
```

Analyze Feature Types and Ranges:
---------------------------------
Next is to explore the data I've retrieved. Run summary function on track features dataframe to explore the columns, datatypes, means, and ranges of the possible explanatory variables for our chosen response variable genre.
```{r, Analyze Features, echo = FALSE}
# Summary
summary(playlists_tracks_features_df_clean)
# TODO: remove really long track from riddim
```

Spotify API Audio Features Description Source: <br>
- https://developer.spotify.com/documentation/web-api/reference/get-audio-features

Possible Numeric Predictor Variables & Ranges: <br>
- Popularity: 0-100 <br>
- Duration: 0-inf <br>
- Danceability: 0-1 <br>
- Energy: 0-1 <br>
- Key: 0-11? <br>
- Loudness: -inf-inf <br>
- Mode: 0 or 1? <br>
- Speechiness: 0-1 <br>
- Acousticness: 0-1 <br>
- Instrumentalness: 0-1 <br>
- Liveness: 0-1 <br>
- Valence: 0-1 <br>
- Tempo: 0-inf <br>
- Time_Signature: 1-5?

Possible Categorical Predictor Variables: <br>
- key_name <br>
- mode_name <br>
- key_mode

Boxplots of Features by Genre:
------------------------------
To analyze the numeric features of tracks I will first create boxplots of each feature by genre. What I'm hoping to see is inter genre variance that would allow for features to be used to explain differences in genres. I utilized the plot_grid() function from cowplot an gridExtra libraries to arrange plots. <br>

Sources: <br>
- http://www.sthda.com/english/wiki/wiki.php?id_contents=7930
```{r Boxplots, fig.width = 10, fig.height = 20}
# Create boxplot of each numeric feature by genre
numeric_features = c("track.popularity", "track.duration_ms", "danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "time_signature")
plots = list()
for (numeric_feature in numeric_features) {
  p = ggplot(data = playlists_tracks_features_df_clean, mapping = aes_string(y = numeric_feature, x = "genre", fill = "genre")) + 
    geom_boxplot() +
    guides(fill = FALSE) +
    scale_fill_manual(values = playlists_info_df_clean$color) +
    labs(title = paste(numeric_feature, "by Sub Genre"), 
         x = "Sub Genre", y = numeric_feature
         )
  plots[[numeric_feature]] = p
}

plot_grid(plotlist = plots, ncol = 2, nrow = 7)
```

Ridgeline Plots of Features by Genre:
-------------------------------------
To better assess the distributions of numeric features I decided to also explore a new plot called a joyplot or ridgeline plot. A ridgeline plot is visual graphic that is essentially a stacked kernel density estimate (KDE) which contains a unique factor for each row of KDE. This enables me to look at a lot of numeric vs factor data in a well summarized format. To create these visuals I used the geom_joy() layer from the library GGjoy which builds on top of the ggplot interface. <br>

GGjoy Sources: <br>
- https://katherinemwood.github.io/post/joy/
```{r Joyplots, fig.width = 10, fig.height = 20}
# Create joyplots of each numeric feature by genre
plots = list() # Init empty list for plots
for (numeric_feature in numeric_features) {
  p = ggplot(data = playlists_tracks_features_df_clean, mapping = aes_string(x = numeric_feature, y = "genre", fill = "genre")) + 
    geom_joy(scale = 2) + # Joyplot layer
    guides(fill = FALSE) + # Remove color legends
    scale_fill_manual(values = playlists_info_df_clean$color) + # Manual color selection
    labs(title = paste("Distribution of",numeric_feature, "by Sub Genre"),
         x = numeric_feature, y = "Sub Genre",
         color = "Sub Genre"
         )
  plots[[numeric_feature]] = p # Add plot to list of plots
}

# Plot list of plots in grid
plot_grid(plotlist = plots, ncol = 2, nrow = 7)
```

Boxplot & Joyplot Plot Analysis: 
--------------------------------
- Popularity: There is significant variance in popularity of tracks within the playlists by genre. House is most popular with Riddim being least popular. The Techno and DnB playlists must be including more new music or lesser known artists in their lists as they both have second modes at 0.
- Duration: Theres little variance in duration by genre. Techno has a wider range and DnB has slightly longer songs.
- Danceability: Techno, Riddim, and House have greater danceability while the remainder have similarly lower danceability.
- Energy: All genres are high in energy (its electronic music) with House being slightly lower and having greater variance.
- Key: Key varies widely among all genres. Riddim has the lowest mean key and House has the smallest IQR of key.
- Loudness: Riddim, Hardstyle, Dubstep, and DnB have greater loudness and House, Techno, and Trance are lower in loudness.
- Mode: ?
- Speechiness: All genres are low in speechiness (its electronic music) with Riddim having the greatest mean and variance.
- Acousticness: All genres are low in acousticness (its electronic music) with House having the greatest mean and variance.
- Instrumentalness: Most genres are lower in instrumentalness with the exception of Trance and Techno. Techno in particular stands out with greater instrumentalness.
- Liveness: Genres such as House and Techno express lower liveness.
- Valence: All genres have similarly low valence, but House has a slightly greater mean than others.
- Tempo: Major difference in means by genre. Genres are strongly defines by their tempo. House has lowest tempo. DnB followed by Hardstyle have greatest tempo.
- Time_Signature: ?

Takeaway Inferences:
--------------------
- The variables that appear to best differentiate all genres from one another ones with high inter-genre variance. These are popularity, loudness, and tempo. 
- Some variable are good at splitting genres into two subgroups as higher vs lower. Variables such as danceability.
- Many other variables were good at differentiating singular genres from others. These include longer duration for Techno, lower energy for House, lower key for Riddim, higher speechiness for Riddim, higher acousticness for House, higher intrumentalness for Techno and Trance, lower liveness for House and Techno, higher valence for House.
- Some variables had little value for inference in difference between genres such as mode and time_signature and will not be excluded as explanatory features.
- I will to not use popularity as an explanatory variable as it is a property that is not necessarily inherent to the song itself. Other audio features come from algorithmic analysis of music files by Spotify and are more objective measures.

2D Scatter of Features:
-----------------------
Given the feature inferences above, I determined the simplest way to differentiate genres by just twoo numeric feature variables would be a tempo vs loudness scatterplot colored by genre. We observe that these two explanatory variables alone do a decent job separating the genres into clusters. However, there's still significant overlap between a few of the genres. Dnb and Hardstyle are easier to distinguish, but genres such as Trance, Techno, and House exist in similar spaces. In particular, Techno appears to have the least tight clustering by this method.
```{r 2D Features Scatterplot, echo = FALSE, fig.width = 8, fig.height = 6}
# Scatter loudness vs tempo colored by genre
ggplot(data = playlists_tracks_features_df_clean, mapping = aes(y = loudness, x = tempo, color = genre)) + 
  geom_point() + 
  scale_color_manual(values = playlists_info_df_clean$color) + # Manual color selection
  labs(title = "Track Tempo vs Loudness by Sub Genre",
       x = "Tempo", y = "Loudness",
       color = "Sub Genre"
       )
```

Fixing Spotify Tempo Error:
---------------------------
I noticed that Spotify's audio feature algorithm might struggle with accurately identifying the tempo of some DnB and Riddim songs. There appears to be a bimodal distribution of tempo for these two genres with the second mode at exactly half the primary mode tempo. The algorithm is likely mis-identifying every other beat. To fix this I could remove the erroneous tracks or adjust their values. I decided to fix the data by doubling the tempo of rows where tempo is less than 90. In the new scatterplot, this seems to have worked perfectly in aligning songs back with their cluster and removing the second tempo modes.
```{r Data Fixing 2D Scatter, echo = FALSE, fig.width = 8, fig.height = 6}
# Double the value of tempo if less than 80
playlists_tracks_features_df_clean = playlists_tracks_features_df_clean |>
  mutate(tempo = if_else(tempo < 90, tempo * 2, tempo))

# Scatter loudness vs tempo colored by genre w/ new fixed data
ggplot(data = playlists_tracks_features_df_clean, mapping = aes(y = loudness, x = tempo, color = genre)) + 
  geom_point() + 
  scale_color_manual(values = playlists_info_df_clean$color) + # Manual color selection
  labs(title = "Track Tempo vs Loudness by Sub Genre (Adjusted Tempo Error)",
       x = "Tempo", y = "Loudness",
       color = "Sub Genre"
       )
```

Principal Component Analysis (PCA) of Features:
-----------------------------------------
Now I'll perform principal component analysis on the track features to reduce the dimensionality from 11 columns while maximizing variance between the components. Principal component analysis, PCA for short, is a technique used to reduce the dimensionality of data while still preserving as much of the differences in the data as possible. In other words, its a method to reduce the number of columns in a dataset. This is achieved by creating new columns known as principal components which are weighted linear combinations of the original columns. The weights, properly known as loadings, are tuned to maintain as much of the variance as possible. However, of course, some variance is lost through this process. Additionally, the new components are somewhat more abstract in nature as they don't singularly represent the original features. Loadings can be analyzed to better infer the values of each component. I have created what is known as a scree plot which is a simple line graph that represents the additional proportion of variance explained by including each additional principal component. Typically, each additional component explains less variance in the data than the last so there is a tradeoff between the complexity or number of components and preservation of variance. To select the ideal number of components you usually look for a sharp elbow bend in the graph. This indicates the point where new components provide diminishing returns in preserving variance. For my data, this occurs at N = 3 principal components. So I will reduce the dimensions of my data from 11 to 3. Conveniently, this will allow for visualization with 3D scatterplots.

PCA Sources: <br>
- https://www.geeksforgeeks.org/principal-component-analysis-with-r-programming/
```{r PCA, fig.width = 6, fig.height = 4}
# Select numeric explanatory features
playlists_tracks_features_df_clean_explanatory = playlists_tracks_features_df_clean |>
  select(track.duration_ms, danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

# PCA
pca_model = prcomp(playlists_tracks_features_df_clean_explanatory, scale. = TRUE)

# Extract Variances from PCA Model
variance_explained <- pca_model$sdev^2 / sum(pca_model$sdev^2)

# Plot PCA Scree
plot(variance_explained, type = "b", xlab = "Principal Component", ylab = "Proportion of Variance Explained", main = "Scree Plot")
```

Explore PCA Loadings and Components:
------------------------------------
Lets look at the resulting loadings and newly transformed data from the PCA. The loadings dataframe shows the weights used for each column to create each principal component. A negative value indicated a negative correlation or impact on the component by that feature and vice versa for positive values. For example, principal component 1 would be maximized by a track that has high values in danceability and loudness and low values in energy, loudness, speechiness, and liveness.  <br>
We can also look at our data in terms of the newly transformed dimensions. Each track's feature values are fed into the model where linear algebra determines the values of its new components. Tracks are now represented by these new components instead of their original features.
```{r PCA Exploration}
# Show loadings for first 3 components from PCA model.
loadings_matrix <- pca_model$rotation
loadings_3 <- loadings_matrix[, 1:3]
# Check loadings df
print(as.data.frame(loadings_3))

# Extract first 3 principal components from PCA model.
pca_components <- as.data.frame(pca_model$x[, 1:3])

# Add genres and colors to components df
pca_components = pca_components|> 
  mutate(genre = playlists_tracks_features_df_clean$genre, color = playlists_tracks_features_df_clean$color)
# Check components df
print(head(pca_components))
```

Interactive 3D Scatter Plot w/ Concentration Ellipsoids:
--------------------------------------------------------
With the new principal components representing the tracks in just three features we can explore the differences by genre using a 3D scatterplot. Using the plot3d() function from the rgl library, I can create interactive 3D scatter plots of track components with color and concentration ellipsoids by genre powered by OpenGL. Because some genres didn't cluster as well as other or overlapped in cluster I decided to subset the chosen genres for increased clarity of visualization. I found that the combination of DnB, House, Riddim, and Techno had most inter genre variance by the principal components. Dubstep had a wide cluster indicating the genre may posess high intra-genre variance. 

rgl Sources: <br>
- https://r-graph-gallery.com/3d_scatter_plot.html <br>
- http://www.sthda.com/english/wiki/a-complete-guide-to-3d-visualization-device-system-in-r-r-software-and-data-visualization <br>
- https://cran.r-project.org/web/packages/rgl/rgl.pdf

```{r 3D Component Scatterplot, fig.height = 10, fig.width = 10}
# TODO: turn into function and create multiple 3d plots with different subsets of genres

# Subset data to make visuals less cluttered
genres_to_subset = c("DnB", "House", "Riddim", "Techno") # , "Hardstyle", "Trance", "Dubstep")
playlists_info_df_clean_subset = playlists_info_df_clean |>
  filter(genre %in% genres_to_subset)
pca_components_subset = pca_components |>
  filter(genre %in% genres_to_subset)

# Set render viewpoint where data looks best
elevation = 22.5 # tweak, rotates around z-axis (PC3)
azimuth = 45 # tweak, rotates around y-axis (PC2)
view3d(theta = azimuth, phi = elevation, fov = 60, zoom = 0.8)

# Create 3D scatter using plot3d() interface from rgl
plot3d(x = pca_components_subset$PC1, y = pca_components_subset$PC2, z = pca_components_subset$PC3, 
       col = pca_components_subset$color, # Color of points
       type = 's', # Point type
       size = 4, # for if type = 'p'
       radius = 0.1, # for if type = 's'
       xlab = "PC1", ylab = "PC2", zlab = "PC3",
       box = FALSE, # Turn off front grid axis
       xlim = c(-2.5, 4), ylim = c(-3, 4), zlim = c(-2.5, 2.5) # Axis ranges
       )

# Create concentration ellipsoids for subset genres
for (row in 1:nrow(playlists_info_df_clean_subset)) {
  # Subset genre for each loop
  pca_components_genre_subset = pca_components_subset |>
    filter(genre == playlists_info_df_clean_subset[row, "genre"])
  # Create ellipsoid
  ellips = ellipse3d(cov(cbind(pca_components_genre_subset$PC1, pca_components_genre_subset$PC2, pca_components_genre_subset$PC3)), 
                   centre = c(mean(pca_components_genre_subset$PC1), mean(pca_components_genre_subset$PC2), mean(pca_components_genre_subset$PC3)),
                   level = 0.50
                   )
  # Add concentration ellipsoid to plot
  plot3d(ellips, col = playlists_info_df_clean_subset[row, "color"], alpha = 0.2, add = TRUE, box = FALSE, type = "shade")
}
# Display 3d scatter plot in rmd
rglwidget()

# Save rotating gif to working dir
# Importing webshot2 seems to be significantly slower than whatever was being used before to render gif. Took 6 minutes! 25 frames/min!
#snapshot3d("3dscatter.png")
#movie3d(spin3d(axis = c(0,0,1), rpm = 4), duration = 15, dir = "./")

# Save Widget
htmlwidgets::saveWidget(rglwidget(width = 1080, height = 1080), 
                        file = "HtmlWidget3dscatter.html",
                        libdir = "./",
                        selfcontained = FALSE
                        )
```

Split Train and Test Sets:
--------------------------
Before we feed the track feature data into a model, we should split our dataset into a train and test set. The goal of this is to reduce the likelihood of overfitting with methods like early stopping and evaluating the model's true predictive accuracy on data the model was not trained on. 

caret Source: <br>
- https://cran.r-project.org/web/packages/caret/vignettes/caret.html
```{r Test Train Split}
# Select x and y cols for model.
model_data_df = playlists_tracks_features_df_clean |>
  select(genre, track.duration_ms, danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

# Test train split to reduce overfitting and accurately assess model predictive performance.
set.seed(24) # for reproducability of randomness 
indices = createDataPartition(model_data_df$genre, p = 0.75, list = FALSE) # 75% train, 25% test split
train = model_data_df[indices, ] # select train rows
test = model_data_df[-indices, ] # select test rows

# format for xgboost
dtrain = xgb.DMatrix(data = as.matrix(train[, -which(names(train) == "genre")]), label = as.numeric(as.factor(train$genre)) - 1)
#dval = xgb.DMatrix(data = as.matrix(test[, -which(names(test) == "genre")]), label = as.numeric(as.factor(test$genre)) - 1)
```

Train Multiclass Classification Model with XGBoost:
---------------------------------------------------
XGBoost is a powerful machine learning library that exists in many languages which was developed to create decision trees. Here I will use the xgboost library with the multi:softmax objective to train a multiclass classification model. In simpler terms, I will train a model which will attempt to predict the correct of 7 genres for a track using 11 selected numeric features. I decided to settle on using 5 rounds for training as testing models with greater numbers of rounds did not result in significantly better accuracy. With more time I would like to implement early stopping to algorithmically optimize the number of rounds run to train the model and provide additional visuals plotting the training process. I'd also like to try tuning hyperparameters like max depth, min child weight, etc.

xgboost Sources: <br>
- https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html <br>
- https://cran.r-project.org/web/packages/xgboost/xgboost.pdf
```{r xgboost Classification Model}
#model = xgboost(data = dtrain, nrounds = 15, objective = "multi:softmax", num_class = 7)

# Parameters for model
param = list(objective = "multi:softmax", num_class = 7)

# Train model
model = xgb.train(params = param, data = dtrain, nround = 5)

# Plot training performance TODO out of time
#plot(model$evaluation_log$validation_0$error, type = "l", xlab = "Number of Rounds", ylab = "Validation Error")

# view model
print(model)
```

XGBoost Model Evaluation:
-------------------------
- Now we need to evaluate the multi classification model we trained using xgboost. The first step is to generate genre classification predictions for the test set features using the xgboost model. Then we can evaluate the accuracy of these predictions by comparing the model predictions to the true response genre. I found this version of the xgboost model to be 71.01% accurate with little tuning required. It is possible to achieve greater accuracy with finer tuning of model parameters. For now though, 71% is a great accuracy rate for a multiclass classification problem with 7 classes and a small dataset! <br>
- We can also generate a confusion matrix to see specifically how each genre was classified and misclassified. DnB, Hardstyle, and House were the more accurate genres to classify with only 2, 6, and 9 misclassified tracks respectively. On the other end, Dubstep was the most challenging class for the model to predict with under a 50% accuracy rate. This finding would concur with some of the inferences above, indicating high intra-genre variance in features for dubstep. <br>
- Lastly we can evaluate the model's feature importance. In particular, we're most interested in gain of each feature as it measures the feature's improvement in model performance. Cover and Frequency measure the proportion of the dataset the feature is responsible for splitting and number of times a feature is used in splitting across all generated trees. For gain, by far the most important feature is tempo. The next three most important features are loudness, danceability, and track duration. These feature importances line up exactly with the inferences I interpreted from the boxplots and ridgelineplots above. The less valuable remaining features include, in order of greatest to least importance, instrumentalness, energy, speechiness, acousticness, valence, and liveness.
```{r Model Eval}
# Calc predictions from model and test set
predictions = predict(model, as.matrix(test[, -which(names(test) == "genre")]))

# Calc accuracy of model predictions to true response
accuracy <- mean(predictions == as.numeric(as.factor(test$genre)) - 1)
print(paste("Accuracy:", accuracy))

# Confusion Matrix
confusion_matrix <- table(test$genre, predictions + 1)
print(confusion_matrix)
# TODO: add columns for accuracy by genre

# Feature Importance
feature_importance = xgb.importance(model = model)
print(feature_importance)
```

Conclusion:
-----------
This notebook dives into inference and predictive modeling of audio features by electronic music sub genre. I found that Spotify generated audio features are in-fact capable of providing valuable inference into differences in electronic music sub genres as well as usefulness in multiclass classification modeling to predict sub genre of songs. Using the Spotify API I was able to engineer my own dataset relevant to my domain interest/knowledge within R. I was also able to exploratory data analysis to provide accurate inferences of track features by sub genre to further clean and select features to be used for principal component analysis and multiclass classification modeling. The features with the most inter sub genre variance included tempo, loudness, and danceability. The audio features, when dimensionality was reduced using PCA, were capable of displaying effective 3D interactive visuals clustering tracks by sub genre aided by concentration ellipses. Finally, using XGBoost to train a decision tree for multiclass classification using features to predict sub genre as the response, the model yielded a 71% accuracy rate on the validation set. Feature importance analysis of the model revealed that the most important features were the same as found by inference; tempo by a large margin followed by loudness, danceability, and track duration. <br>
There are parts of this study I would like to expand upon such as analyzing key and mode by genre, more descriptive statistics of numeric features by genre, turning the 3D scatterplot into a function to produce additional interactive plots with other combinations of genres, generalizing the code so others could input their own playlists of interest for automatic analysis, using the xgboost model to automatically separate an unfiltered playlist of music into algorithmically sorted new playlists so a user doesn't have to manually, and or additional visuals for the xgboost model. However, these ideas would take significantly more time to implement so I must call the study here for submission. Thank you for reading!
