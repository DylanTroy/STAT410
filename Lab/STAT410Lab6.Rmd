---
title: "STAT 410 Lab 6"
author: "Kristin Duncan"
date: ""
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1

Derive the least squares estimates for $\beta_0$ and $\beta_1$ in the simple linear regression model.  That is, show that \[\sum_{i=1}^n(y_i-(b_0+b_1x))^2\] is minimized when \[b_1=\frac{SS_{xy}}{SS_{x}}=\frac{\sum_{i=1}^n x_iy_i-\frac{1}{n}\sum_{i=1}^nx_i\sum_{i=1}^ny_i}{\sum_{i=1}^nx_i^2-\frac{1}{n}\left(\sum_{i=1}^nx_i\right)^2} \] and \[b_0=\overline{y}-b_1\overline{x}.\]  

You can upload an image of your handwritten work.



## Exercise 2
Load the Survey package and access the apisrs data.  Check out all the variable definitions by putting apipop into the help search
a. Create a filtered data set containing only the elementary schools.  Call this data set apiel.  Use it for the rest of this problem.
b. Create two correlation matrices with ggpairs to look at how api00 relates to the other numeric variables in the data set.  One matrix should include api00 with columns 11, 20, 21, 23-26, and 34-36.  The other should include the parental education variables in columns 27-30. Do not consider api99, target, or growth in this analysis.  Comment on which variables are strongly associated and whether the association is positive or negative.
c. Create a linear regression model predicting api00 with meals.  Output the summary data.
d. Check the assumptions about residuals graphically.  Comment on whether the linear model seems appropriate here.
e. Conduct a hypothesis test of the null that the slope is zero versus the alternative that the slope is negative.  Include all steps of the hypothesis test.
f. Plot api00 versus meals using ggplot.  Include the confidence band for the line.  Add in the prediction ribbon for new observations using 95% confidence.
g. Compute a 90% prediction interval for a school where 50 percent of students receive meals.  Give a sentence interpreting this interval.
h. Identify the 5 schools with percent receiving meals over 50 that have the largest positive residuals.  These schools are doing well and may have programs others can emulate.


```{r ex2, warning=F}
library(tidyverse)
#install.packages("GGally")
library(GGally)
#install.packages("survey")
library(survey)
data(api, package="survey")
# View(apisrs)
```


```{r, warnings=F}
# 2. a.
apiel = apisrs |>
  filter(stype == "E")
# View(apiel)
```


```{r, message=F, warning=F}
# 2. b.
plot1 = ggpairs( select(apiel, api00, 11, 20, 21, 23:26, 34:36) )
ggpairs( select(apiel, api00, 27:30) )

# First ggpair correlation matrix: The numeric variables with a strong linear correlation (0.7, 1.0) with api00 are meals and ell with r values of -0.893 and -0.714 respectively. The variables meals and ell were negatively correlated with api00. Variables with a moderate to lower correlation (0.3, 0.7) were full, emer, and mobility with r values of 0.441, -0.395, and -0.308 respectively. The variables emer and mobility were negatively correlated with api00. The remaining numeric variables had low to no linear correlation (0.0, 0.3). 
# Second ggpair correlation matrix: The numeric variable with a strong linear correlation (0.7, 1.0) with api00 is not.hsg with an r value of -0.739 and is negatively correlated. hsg has a moderate negative linear correlation (0.3, 0.7) with and r value of -0.434. The remaining numeric variables have little to no linear correlation (0.0, 0.3) with api00.

# saved plot for better readability.
ggsave("ggpairs_output.png", plot1, width = 10, height = 10, units = "in")
```


```{r, warnings=F}
# 2. c. 
model = lm(api00 ~ meals, data = apiel)
summary(model)
```


```{r, warnings=F}
# 2. d.
linear_residuals = resid(model)
#check for homoscedasticity (constant variance of residuals) and independence of errors.
plot(linear_residuals)
#check for normality
plot(density(linear_residuals), main = "Density Plot of Residuals")
shapiro.test(linear_residuals)$p.value
#plots for linear model
plot(model)
# The linear model for api00 with respect to meals appears appropriate according to the residual plots. There are no discernible patterns to be observed among the residual points and the normal qq plot follows an approximately straight line. 
```


```{r, warnings=F}
# 2. e. 
summary(model)
#get test statistic
t_stat = summary(model)$coefficients['meals','t value']
#get p-value
p_val = summary(model)$coefficients['meals','Pr(>|t|)']
confint(model)
# Hypothesis Test Steps:
# 1. H0: the slope of the linear model is b1= 0  Ha: the slope of the linear model is negative b1< 0 
# 2. Based on the above test (d) we can conclude that the assumption of normality is met, we also know the data is independent.
# 3. The test statistic (F-value) is `r t_stat`
# 4. p-value is `r p_val` which is nearly 0.
# 5. Since the p-value of `r p_val`  is less than the level of significance (0.05) we can conclude there is sufficient evidence to reject the null and conclude the slope of the linear model is negative (<0).

# The estimate for the slope of the linear regression of api00 with respect to meals is -3.9667 with a p-value of 2.2*10^-16 and has a 95% conf int of (-4.30, -3.63). Because the p-value is significantly lower than the 0.05 level of significance alpha, as well as because the conf int does not contain 0, we have enough evidence to conclude that the slope of the linear regression is negative with statistic significance.
```


```{r, message=F, warnings=F}
# 2. f.
pred.int = as.data.frame(predict(model, interval = "prediction"))
apiel1 = bind_cols(apiel, pred.int)


ggplot(data = apiel1, mapping = aes(x= meals, y= api00)) +
  geom_point() +
  stat_smooth(method = "lm", level = 0.95) + 
  geom_line(aes(y = lwr, x=meals), color = "red", linetype = "dashed") +
  geom_line(aes(y = upr, x=meals), color = "red", linetype = "dashed") +
  labs(
    title = "meals vs api00",
    subtitle = "with 95% conf band and prediction ribbon",
    x = "meals", y = "api00"
  )
```


```{r, message=F, warnings=F}
# g. Compute a 90% prediction interval for a school where 50 percent of students receive meals.  Give a sentence interpreting this interval.
# 2. g.
meals_50 = data.frame(meals = c(50))
predict(model, newdata =  meals_50, interval = "prediction", level = 0.9)
# We can interpret this to mean that the actual the value of api00 (Academic Performance Index for 2000) for a school where 50 percent of students receive meals will fall between between 582 and 786.
```


```{r, message=F, warnings=F}
# h. Identify the 5 schools with percent receiving meals over 50 that have the largest positive residuals.  These schools are doing well and may have programs others can emulate.
# 2. h.
#add residuals to the a copy of the apiel dataframe
apiel_res <- bind_cols(apiel, residuals = linear_residuals)

#filter to get only schools recieiving meals over 50 and arrange by largest residuals
top_schools = apiel_res |>
filter(meals > 50) |>
arrange(desc(residuals))

#print top 5
top_schools[1:5,]
```



## Exercise 3

Continuing with the apiel dataset, consider predicting ell with meals. The relationship between meals and ell shows some nonlinearity.  Find a transformation that produces a higher R^2 value without causing problems in the residuals.  Verify this graphically and comment on the improvement.  Note that if you add a new (transformed) variable to the data set, it's fine to use this new name in the formula.  If you include the math operators for the trasformation in your formula, you should use the AsIs operator I() around your expression so the math operators are not misinterpreted in the formula.


```{r}
# 3. 
fit = lm(ell ~ meals + I(meals^2) + I(meals^3), data = apiel)
# Multiple R^2 = 0.5278 with non normal or random residuals with no transformations.

# failed attempt to plot transformed variable...
# apiel$meals_trans = apiel$meals + I(apiel$meals)^2 + I(apiel$meals)^3
# ggplot(data = apiel, mapping = aes(x= meals_trans, y= ell)) + 
#  geom_point() +
#  geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], color = "blue") +
#  labs(
#    title = "meals vs ell",
#    subtitle = "with linear regression",
#    x = "transformed meals", y = "ell"
#)
summary(fit)
par(mfrow = c(2, 2))
plot(fit, which = 1)
plot(fit, which = 2)
plot(fit, which = 3)
plot(fit, which = 5)

# The transformation that produces the highest R^2 value is the cubic transformation with a multiple R^2 of 0.5891. 
```


```{r}
```
## Exercise 4

a. Fit a linear model called edmodel predicting api00 with average parent education (avg.ed).
b. Give 95% confidence intervals for the slopes and intercepts of this model.  The matrix summary(edmodel)$coefficients has the estimates in the first column and their standard errors in the second column.
c. Evaluate the fit and predictive ability of this model.  ie, how far off do you expect predictions to be on average, and what fraction of the variability in api00 can be explained by the regression on avg.ed?
d. Calculate $(X'X)$ for this model.  Identify what values fall in each of the four slots  (what formula would go in each generally)?

```{r}
# 4. a.
edmodel = lm(api00 ~ avg.ed, data = apiel)
summary(edmodel)

# 4. b.
confint(edmodel)
# The 95% conf int for the intercept is (194.57, 280.99) and for slope is (140.84, 171.15)

# 4. c. 
ggplot(data = apiel, mapping = aes(x= avg.ed, y= api00)) + 
  geom_point() +
  geom_abline(intercept = coef(edmodel)[1], slope = coef(edmodel)[2], color = "blue") +
  labs(
    title = "api00 vs avg.ed",
    subtitle = "with linear regression",
    x = "avg.ed", y = "api00"
  )
par(mfrow = c(2, 2))
plot(edmodel, which = 1)
plot(edmodel, which = 2)
plot(edmodel, which = 3)
plot(edmodel, which = 5)
# The linear regression has an r^2 value of 0.7543 meaning that there is a strong positive linear correlation. We expect predictions of api00 to be off by 67.39 on average, the residual standard error. The percent of variability in api00 explained by the regression model with avg.ed is 75.43%. With random and normal residual plots for the model, we can say that this model has good predictive ability.

# 4. d.
Xpx <- t(model.matrix(edmodel)) %*%
   model.matrix(edmodel)
print(Xpx)
# Slot (1,1) or 137.00 represents the number of observations in the dataset, slot (1,2) and slot (2,1) represents the sum of the balues in the avg.ed column. Finally slot (2,2) represents the sum of the squared values in the avg.ed column. 
```

